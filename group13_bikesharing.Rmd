---
title: "Predicting the Number of Bike Shares"
output:
  html_document:
    toc: true
    theme: united
---

In this analysis we would like to predict the count of bike rentals in Washington DC with a regression analysis, Decision Trees, Support Vector Machines (SVMs) and Neural Networks (NN).

# 0. Import Packages
Before the project starts, all required packages are imported.
```{r, message=FALSE}
library(ggplot2)
library(e1071)
library(mgcv)
library(corrplot)
library(tree)
library(mgcv)
library(tidyverse)
library(boot)
library(MASS)
library(ipred)
library(randomForest)
library(gbm)
library(modelr)
```

```{r message=FALSE, include=FALSE}
set.seed(123)
```

# 1. Getting Data
## Data source
We downloaded the dataset "bikesharing.csv" from https://code.datasciencedojo.com/datasciencedojo/datasets/tree/master/Bike%20Sharing.

## Loading the data
In a first step the dataset is imported to R and stored in the data.frame *d.bike*:
```{r}
d.bike <- read.csv("bikesharing.csv", header=TRUE)
```

## Describing the dataset
```{r include=FALSE}
str(d.bike)
```

The dataset contains hourly information about a day and its weather conditions in 17379 observations of 17 variables. In the following, the individual attributes will be explained:

* *instant*: record index
* *dteday*: Date
* *season*: 1 = spring, 2 = summer, 3 = fall, 4 = winter
* *yr*: Year; 0: 2011, 1: 2012
* *mnth*: Month; 1 to 12
* *hr*: Hour; 0 to 23
* *holiday*: 0 = no holiday, 1 = holiday
* *weekday*: Day of the week; 1 to 7
* *workingday*: 0 = weekend or holiday, 1 = working day
* *weathersit*:
  + 1: Clear, Few clouds, Partly cloudy, Partly cloudy
  + 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
  + 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
  + 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog 
* *temp*: Normalized temperature in Celsius; The values are derived via (t-t_min)/(t_max-t_min),   t_min=-8, t_max=+39;
* *atemp*:  Normalized feeling temperature in Celsius; Normalized   feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50
* *hum*: Normalized relative humidity; The values are divided to 100 (max); 
* *windspeed*: Normalized windspeed ; The values are divided to 67 (max)
* *casual*: number of non-registered  (eg. tourists)
* *registered*: number of registered users (eg. workers that use bikes daily)
* *cnt*: number of total rentals

## Investigating the correlation between all predictors
In order to make a valid feature selection in the next step, we now want to investigate the correlation between the predictors. The so-called corrplot offers a simple and clear display. This shows a correlation matrix. Correlations are represented by dots. The exact value can be read off the scale.
```{r}
predictors = c("dteday", "season", "yr","mnth","hr","holiday","weekday",
             "workingday","weathersit","temp","atemp","hum","windspeed", "casual", "registered")
corrplot(cor(data.matrix(d.bike[predictors])))
```

As the corrplot shows, dteday correlates strongly with the predictor yr. Furthermore it can be seen that there is a correlation between dteday and season as well as mnth. There is also a relatively strong correlation between month and season.The strongest correlation of almost 1 shows temp and atemp.

## Feature Selection
We decide to delete the feature *instant* and *dteday*. *instant* is a record index that has been artificially created. Therefore it will have no relevant effect on *cnt*. *dteday* is the date of the record. It correlates with the three other attributes *season*, *yr* and *mnth*. The features season and mnth as well as temp and atem will be kept for now.

```{r}
d.bike <- subset(d.bike, select=-c(instant, dteday))
```

```{r include=FALSE}
d.bike$season <- as.factor(d.bike$season)
d.bike$yr <- as.factor(d.bike$yr)
d.bike$mnth <- as.factor(d.bike$mnth)
d.bike$hr <- as.factor(d.bike$hr)
d.bike$holiday <- as.factor(d.bike$holiday)
d.bike$weekday <- as.factor(d.bike$weekday)
d.bike$workingday <- as.factor(d.bike$workingday)
d.bike$weathersit <- as.factor(d.bike$weathersit)
```

## Checking the data
```{r}
head(d.bike)
tail(d.bike)
```

As it looks like the data set was imported completely. To check if there are any missing values (not available, NA) in this dataset we count the number of NAs in the data set.
```{r}
sum(is.na(d.bike))
mean(is.na(d.bike))
```

The record contains no rows with missing data.

## Creating training and test data set
At this point the data set d.bike is divided into training and test set. The training set should consist of 75% of the data and the test set of 25% of the data.
```{r}
d.bike.train.id <- sample(seq_len(nrow(d.bike)),size = floor(0.75*nrow(d.bike)))
d.bike.train <- d.bike[d.bike.train.id,]
d.bike.test <- d.bike[-d.bike.train.id,]
```

# 2. Explorative Data Analysis
In this section each attribute is considered to make certain assumptions. Different statistical diagrams are used for this purpose. The effects of the different attributes on cnt are tested using ANOVA for categorical attributes and a linear regression model for numerical values. The aim of this explorative data analysis is to get a starting point for the model building.

To keep our documentation clear, not all graphical analyses are shown. For example, interactions between predictors are not shown in this document but only described. At these points, references are made to the R-file.

## Count
In the very first step we would like to look at the distribution of the response variable count. For this purpose we create a histogram.
```{r}
ggplot(data = d.bike, aes(x=cnt)) +
  geom_histogram(bins=30, colour="black", ylab="Frequency") + xlab("Count") + ylab("Frequency")
mean(d.bike$cnt)
sd(d.bike$cnt)
```
As we can see, this is a right skewed gamma distribution. So in most hours - the number of bike rentals are hourly observations - between 0 and 30 bikes (the most left bar) are rented.

Bootstrapping:
```{r include=FALSE}
sort(d.bike$cnt)
```

```{r}
mean(d.bike$cnt)

id <- sample(1:length(d.bike$cnt), replace = TRUE)
mean(d.bike$cnt[id])

B <- 1000
t.mean <- c()
for(i in 1:B){
  t.id <- sample(1:length(d.bike$cnt), replace = TRUE)
  t.d.bike <- d.bike$cnt[t.id]
  t.mean[i] <- mean(t.d.bike)
}
length(t.mean)

hist(t.mean, breaks = 50)
abline(v = mean(d.bike$cnt), col = "red")

sorted.means <- sort(t.mean)
quantile(sorted.means, probs = c(0.025, 0.975))
```

## Season
If we want to visualize the effect of the *season* as a categorical variable on the number of bike rentals it is a good idea to use a box plot.
```{r}
ggplot(data = d.bike, aes(group=season, y = cnt, x = as.factor(season))) +
  geom_boxplot() + xlab("Season") + ylab("Count")
```

As the box plot shows, season 1 (spring) is particularly different from season 2, 3 and 4 (summer, autumn and winter). The median is lowest in spring, even lower than in winter. In summer, autumn and winter, the medians differ only marginally.  Since the medians differ partially, one can assume a slight effect. However, we want to examine this more closely.

With an Analysis of Variance (ANOVA) we check whether at least one level of Seasons has a significant effect on the number of bike rentals. The first step is to create a model that does not consider any effect from *season*. With an F-test we compare then both models with each other.

```{r}
lm.season.1 <- lm(cnt ~ as.factor(season), data = d.bike)
lm.season.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.season.0, lm.season.1)
```

As we can see from the p-value there is strong evidence that at least one level in *season* has a strong effect on the number of bike rentals. The lower value of the residual sums of squares in lm.season.1 also indicates that the more complex model has smaller residuals than the less complex model lm.season.0.

## Year
Since we also consider year to be a categorical variable, a box plot is again a good choice. For all categorical variables we will use a boxplot for the graphical representation in the following.

```{r}
ggplot(data = d.bike, aes(group=yr, y = cnt, x = yr)) +
  geom_boxplot()
```

At first glance we can see that the number of bike rentals has increased in year 1 (2012). The median is above the median of year 0 (2011).

```{r}
lm.year.1 <- lm(cnt ~ as.factor(yr), data = d.bike)
lm.year.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.year.0, lm.year.1)
```

The p-value shows strong evidence that one of the two years 2010 and 2011 have a significant effect on cnt. Furthermore, the RSS shows that the more complex model lm.year.1 has significantly smaller residuals than the simpler one.

## Month
```{r}
ggplot(data = d.bike, aes(group=mnth, y = cnt, x = as.factor(mnth))) +
  geom_boxplot() + scale_x_discrete(limits=seq(1,12)) + xlab("Month") + ylab("Count")
```

Here we see that the bike rental increased during the summer months May to September or when the temperatures were more pleasant. Visually, the months seem to have an effect on cnt.

```{r}
lm.month.1 <- lm(cnt ~ as.factor(mnth), data = d.bike)
lm.month.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.month.0, lm.month.1)
```

As the p-value shows, there is strong evidence that at least one month must have a significant effect on cnt. Also the RSS shows little surprising. The more complex model lm.month.1 seems to represent the data better than the model lm.month.0.


## Hour
```{r}
ggplot(data = d.bike, aes(group=hr, y = cnt, x = as.factor(hr))) +
  geom_boxplot() + scale_x_discrete(limits=seq(1,23)) + xlab("Hour") + ylab("Count")
```

The same applies to the time of day and hour. Bikes were most often rented in the morning between 7 and 8 am. and in the evening between 5 and 6 pm. The same flow also applies to the median. The individual times of day seem to differ greatly from one another.

```{r}
lm.hr.1 <- lm(cnt ~ as.factor(hr), data = d.bike)
lm.hr.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.hr.0, lm.hr.1)
```

The p-value also reflects this. At least one time of day has a significant effect on cnt. The RSS differs here extremely strongly between complex and simple models. The time of day will probably play a role in the final regression model.

## Holiday
```{r}
ggplot(data = d.bike, aes(group=holiday, y = cnt, x = as.factor(holiday))) +
  geom_boxplot() + xlab("Holiday") + ylab("Count")
```

During the holiday period, fewer bikes are rented according to the boxplot diagram. However, the difference is very pleasant compared to the normal working period. The holiday period median is close to the work time median.

```{r}
lm.holiday.1 <- lm(cnt ~ as.factor(holiday), data = d.bike)
lm.holiday.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.holiday.0, lm.holiday.1)
```

The p-value shows strong evidence that no holiday and/or holiday has a significant effect on cnt. However, if we look at RSS at this point, which differs only minimally between simple and complex model, holiday could be a feature that will be not included in the model.

## Weekday
```{r}
ggplot(data = d.bike, aes(group=weekday, y= cnt, x = as.factor(weekday))) +
  geom_boxplot() + xlab("Weekday") + ylab("Count")
```

During all seven days of the week, the bike rental is almost constant. On Monday and Sunday there are less bikes rented. The median is lowest on Monday and highest on Saturday. On all other days the median is similar.  

```{r}
lm.weekday.1 <- lm(cnt ~ as.factor(weekday), data = d.bike)
lm.weekday.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.weekday.0, lm.weekday.1)
```

As the p-value shows, there is only medium evidence that at least one weekday has a significant effect on cnt. Also the RSS does not differ too much between simple and complex model. This leads to the conclusion that probably the weekday is not included in the final model.

## Working day
```{r}
ggplot(data = d.bike, aes(group=workingday, y = cnt, x = as.factor(workingday))) +
  geom_boxplot() + xlab("Working day") + ylab("Count")
```

On working or not-working days, the count of the bike rentals are almost equal. This is also identical with the both medians.

```{r}
lm.workingday.1 <- lm(cnt ~ as.factor(workingday), data = d.bike)
lm.workingday.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.workingday.0, lm.workingday.1)
```

As the p-value shows, there is strong evidence that the no working day and/or working day has a significant effect on cnt. Again, the RSS does not differ very much. Working day possibly brings no benefit in the model and will probably be omitted.

## Weather Situation
```{r}
ggplot(data = d.bike, aes(group=weathersit, y = cnt, x = as.factor(weathersit))) +
  geom_boxplot()  + xlab("Weather Situation") + ylab("Count")
```

If we look at the weather divided on a scale between 1 and 4 as shown in the boxplot, we see that the bike rental is steadily decreasing from 1 to 4. The value 1 is the good weather and 4 the bad weather. Whereby from the value 3 away, probably already bad weather, the renting goes down faster. An identical flow can also be observed for the medians, starting from the first box plot 1 to 4.

```{r}
lm.weathersit.1 <- lm(cnt ~ as.factor(weathersit), data = d.bike)
lm.weathersit.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.weathersit.0, lm.weathersit.1)
```

There is strong evidence that at least one weather situation has a significant effect on cnt. The RSS differs greatly between the two models. This will probably be a feature that makes it into the final model.

## Temperature

```{r}
ggplot(data = d.bike, mapping = aes(y = cnt, x = temp)) +
  geom_point()  + xlab("Normalized Temp") + ylab("Count") + geom_smooth()
``` 

The representation of the regression line confirms the positive correlation. As the smoothing spline shows, temperature is a more complex relationship than linearity. It is possible that a Generalised Additive Model (GAM) could be used if temp is considered in the final model. As it seems higher temperatures lead to a higher number of rented bikes.

```{r}
library(mgcv)
gam.temp.1 <- gam(cnt ~ s(temp), data = d.bike)
summary(gam.temp.1)
``` 

First, a relatively high estimated degree of freedom (edf) of 8,747 is striking. This indicates the high complexity of the relationship. The p-value shows strong evidence that the slope is not 0 and thus there is a correlation between temp and cnt. The adjusted R-Squared shows that 17 percent of the variability is explained by the data. Thus temp could play a role in the final model. temp shows interaction effects with all of the categorical variables except the yr (see line 142, group13_bikesharing.R).

## Feeling Temperature

```{r}
ggplot(data = d.bike, mapping = aes(y = cnt, x = atemp)) +
  geom_point() + xlab("Normalized Feeling Temp") + ylab("Count") + geom_smooth()
```

Not surprisingly, atemp is also related to a higher degree of complexity. So again a GAM is useful to investigate the effect of atemp on cnt.

```{r}
gam.atemp.1 <- gam(cnt ~ s(atemp), data = d.bike)
summary(gam.atemp.1)
``` 

The edf again shows a more complex relationship between atemp and cnt. The p-value shows strong evidence that the slope is not 0. The R-Squared shows that the model explains 17% of the data. Therefore, atemp could also play a role in the final model. atemp shows interaction effects with all of the categorical variables except the yr (see line 160, group13_bikesharing.R).

## Humidity

```{r}
ggplot(data = d.bike, mapping = aes(y = cnt, x = hum)) +
  geom_point() + xlab("Humidity") + ylab("Count") + geom_smooth()
```

The smoothing spline shows a possible quadratic effect of hum on cnt. Once again, a GAM is the obvious choice.

```{r}
gam.hum.1 <- gam(cnt ~ s(hum), data = d.bike)
summary(gam.hum.1)
``` 

As the hypothesis test shows, there is strong evidence that hum has an effect on cnt. Furthermore, the R-Squared shows that the model explains 11% of the data. Thus, hum could also be relevant for the final model. hum shows interaction effects with all of the categorical variables (see line 178, group13_bikesharing.R).

## Windspeed

```{r}
ggplot(data = d.bike, mapping = aes(y = cnt, x = windspeed)) +
  geom_point() + xlab("Windspeed") + ylab("Count") + geom_smooth()
```

The graph shows that windspeed might have a linear effect on cnt. Furthermore the smoothing spline shows that there is very little influence of windspeed, because the curve is almost flat.

```{r}
lm.windspeed.1 <- lm(cnt ~ windspeed, data = d.bike)
summary(lm.windspeed.1)
``` 

The wind speed has a positive effect on cnt. This is surprising, since the observed data seems to decrease with increasing wind speed. As R-Squared shows, this regression model explains only 0.8% of the data. This indicates that the attribute can be omitted in the final model.

## Casual Users and registered users

```{r}
ggplot(data = d.bike, mapping = aes(y = cnt, x = casual)) +
  geom_point() + xlab("Casual Users") + ylab("Count") + geom_smooth()
```

```{r}
gam.casual.1 <- gam(cnt ~ s(casual), data = d.bike)
summary(gam.casual.1)
``` 

```{r}
ggplot(data = d.bike, mapping = aes(y = cnt, x = registered)) +
  geom_point() + xlab("Registered Users") + ylab("Count") + geom_smooth()
```

```{r}
lm.registered.1 <- lm(cnt ~ registered, data = d.bike)
summary(lm.registered.1)
``` 

Both casual and registered users show a positive effect and explain a large part of the data with the model. This suggests that casual and registered users need not be considered separately. Furthermore, casual and registered users are not further distinguished and are therefore not considered in the model.

## Model development

In this section, the insights gained from the exploratory data analysis and the consideration of the individual attributes should help to form a hypothesis about what the final model might look like. This hypothesis is the starting point for developing the model.

The explorative data analysis has shown that the attributes season, yr, mnth, hr, weathersit, temp, atemp and hum can probably be considered in the model. From this, the following model can be assumed:

    cnt = season + yr + mnth + hr + weathersit + temp + atemp + hum + season:temp + season:atemp + season:hum + yr:hum + mnth:temp + mnth:atemp + mnth:hum + hr:temp + hr:atemp + hr:hum + weathersit:temp + weathersit:atemp + weathersit:hum 
    
First five models are initialized. The full model with all predictors, the starting model with all interactions, the starting model without considering the interactions, the starting model without considering the polynomial effect and the starting model without considering polynomial and interaction effects.

```{r}
full.model.1 <- cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(holiday) + as.factor(weekday) + as.factor(workingday) + as.factor(weathersit) + temp + atemp + hum + windspeed + casual + registered

starting.model.1 <- cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(weathersit) + poly(hum, degree = 8.7) + poly(temp, degree = 8) + poly(atemp, degree = 8.9) + season:temp + season:atemp + season:hum + yr:hum + mnth:temp + mnth:atemp + mnth:hum + hr:temp + hr:atemp + hr:hum + weathersit:temp + weathersit:atemp + weathersit:hum

starting.model.2 <- cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(weathersit) + poly(hum, degree = 8.7) + poly(temp, degree = 8) + poly(atemp, degree = 8.9)

starting.model.3 <- cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(weathersit) + temp + atemp + hum + season:temp + season:atemp + season:hum + yr:hum + mnth:temp + mnth:atemp + mnth:hum + hr:temp + hr:atemp + hr:hum + weathersit:temp + weathersit:atemp + weathersit:hum

starting.model.4 <- cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(weathersit) + temp + atemp + hum

```

To find out which of the four starting models fits the data better, the results of the four models are first compared.

```{r}
lm.starting.model.1 <- lm(starting.model.1, data = d.bike)
lm.starting.model.2 <- lm(starting.model.2, data = d.bike)
lm.starting.model.3 <- lm(starting.model.3, data = d.bike)
lm.starting.model.4 <- lm(starting.model.4, data = d.bike)
summary(lm.starting.model.1)$adj.r.squared
summary(lm.starting.model.2)$adj.r.squared
summary(lm.starting.model.3)$adj.r.squared
summary(lm.starting.model.4)$adj.r.squared
```

If only the adjusted R-Squared is considered at this point, it is shown that the starting.model.1 with a value of about 0.747 explains the variability of the data better than the other starting models. Thus we take the starting.model.1 as a starting point for the model development.

```{r}
summary(lm.starting.model.1)
```

If now the p-value of the individual predictors and the interaction relationships in the output of starting.model.1 are considered, it becomes clear that certain attributes have no significant effect on cnt. The interaction relationships weathersit:atemp, weathersit:temp, hr:atemp and mnth:temp are deleted from the model. In the second iteration the interaction relations and predictors with a weak and a medium effect are removed from the model. These are: mnth:hum, season:temp and weathersit:atemp. The iteration was performed separately in the analysis (see lines 246-303, group13_bikesharing.R). For reasons of clarity, the predictors and interactions are removed in one pass.

```{r}
##1. iteration
lm.starting.model.1 <- update(lm.starting.model.1, . ~ . - weathersit:atemp)
lm.starting.model.1 <- update(lm.starting.model.1, . ~ . - weathersit:temp)
lm.starting.model.1 <- update(lm.starting.model.1, . ~ . - hr:atemp)
lm.starting.model.1 <- update(lm.starting.model.1, . ~ . - mnth:temp)
##2. iteration
lm.starting.model.1 <- update(lm.starting.model.1, . ~ . - mnth:hum)
lm.starting.model.1 <- update(lm.starting.model.1, . ~ . - season:temp)
lm.starting.model.1 <- update(lm.starting.model.1, . ~ . - weathersit:hum)
summary(lm.starting.model.1)
```

Now all interactions with weak and medium effect are removed from the model. atemp also shows a medium effect. Since it is present in certain interactions with strong effect, atemp is retained. The adapted R-Squared has only been reduced by 0.0003 points due to the adjustments. The final model now has an adjusted R-Squared of 0.7452, which explains about 74.5% of the data. The final model now looks as follows:

    cnt = season + yr + mnth + hr + weathersit + temp + atemp + hum + season:atemp + season:hum + yr:hum + mnth:atemp + hr:temp + hr:hum
    
The predictors temp, atemp and hum show polynomial effects.

```{r include=FALSE}
final.model.1 <- cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(weathersit) + poly(hum, degree = 8.7) + poly(temp, degree = 8) + poly(atemp, degree = 8.9) + atemp:season + hum:season + hum:yr + atemp:mnth + temp:hr + hum:hr
```

## Cross Validation
In this section, the result from the section "Model development" shall be validated by cross-validation. For this purpose, all models that played a role in the development of a final model are compared with each other. The mean R-squared of the individual models, which is calculated from 10-fold cross-validation, serves as a comparison value.

```{r, warning = FALSE}
lm.starting.model.1 <- lm(starting.model.1, data = d.bike)
lm.starting.model.2 <- lm(starting.model.2, data = d.bike)
lm.starting.model.3 <- lm(starting.model.3, data = d.bike)
lm.starting.model.4 <- lm(starting.model.4, data = d.bike)
lm.full.model.1 <- lm(full.model.1, data = d.bike)
lm.final.model.1 <- lm(final.model.1, data = d.bike)

for(i in 1:10){
  d.bike.train.id <- sample(seq_len(nrow(d.bike)),size = floor(0.75*nrow(d.bike)))
  d.bike.train <- d.bike[d.bike.train.id,]
  d.bike.test <- d.bike[-d.bike.train.id,]

  #predict data with starting model 1
  lm.starting.model.1.train <- lm(starting.model.1, data = d.bike.train)
  predicted.starting.model.1.test <- predict(lm.starting.model.1.train,
                                             newdata = d.bike.test)
  r.squared.starting.model.1 <- cor(predicted.starting.model.1.test, d.bike.test$cnt)^2
  
  #predict data with starting model 2
  lm.starting.model.2.train <- lm(starting.model.2, data = d.bike.train)
  predicted.starting.model.2.test <- predict(lm.starting.model.2.train,
                                             newdata = d.bike.test)
  r.squared.starting.model.2 <- cor(predicted.starting.model.2.test, d.bike.test$cnt)^2
  
  #predict data with starting model 3
  lm.starting.model.3.train <- lm(starting.model.3, data = d.bike.train)
  predicted.starting.model.3.test <- predict(lm.starting.model.3.train,
                                             newdata = d.bike.test)
  r.squared.starting.model.3 <- cor(predicted.starting.model.3.test, d.bike.test$cnt)^2
  
  #predict data with starting model 4
  lm.starting.model.4.train <- lm(starting.model.4, data = d.bike.train)
  predicted.starting.model.4.test <- predict(lm.starting.model.4.train,
                                             newdata = d.bike.test)
  r.squared.starting.model.4 <- cor(predicted.starting.model.4.test, d.bike.test$cnt)^2
  
  #predict data with full model
  lm.full.model.train <- lm(full.model.1, data = d.bike.train)
  predicted.full.model.test <- predict(lm.full.model.train,
                                       newdata = d.bike.test)
  r.squared.full.model <- cor(predicted.full.model.test, d.bike.test$cnt)^2
  
  #predict data with final model
  lm.final.model.train <- lm(final.model.1, data = d.bike.train)
  predicted.final.model.test <- predict(lm.final.model.train,
                                        newdata = d.bike.test)
  r.squared.final.model <- cor(predicted.final.model.test, d.bike.test$cnt)^2
}

mean(r.squared.starting.model.1)
mean(r.squared.starting.model.2)
mean(r.squared.starting.model.3)
mean(r.squared.starting.model.4)
mean(r.squared.full.model)
mean(r.squared.final.model)
```

As the comparison shows, the full.model.1 according to R-Squared is the best model. However, since the final model is the second best and much less complex than the full model, the result of the model development can be validated. For further analysis steps final.model.1 is used.

# 3. Regression Analysis
In this chapter the bike rental cnt will be predicted respectively estimated using a linear, a non-linear and a poisson regressive model. The three models will then be compared using a 10-fold cross-validation based on the R-Squared. 

## Linear Regression
```{r}
lm.bike.1 <- lm(final.model.1, data = d.bike)
summary(lm.bike.1)
```
The linear regression model explains about 74.5 of the variability of the data. This value has already been calculated in the section "Model development".

## Non-linear Regression
```{r}
gam.bike.1 <- gam(cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(weathersit) + s(hum) + s(temp) + s(atemp) + atemp:season + hum:season + hum:yr + atemp:mnth + temp:hr + hum:hr, data = d.bike)
summary(gam.bike.1)
```
The non-linear regression model shows the same result as the linear regression model. The variability of the data is explained by 74.5%.

## Poisson Regression

```{r}
poi.bike.1 <- glm(final.model.1, data = d.bike)
summary(poi.bike.1)
```

## Comparing the models
To enable a meaningful comparison between the three models, the models should train 10 times with the training set and then predict the new data. Finally, the corresponding R-Squared of all three models is compared to each other to make a statement about the best model.
```{r, warning = FALSE}
for(i in 1:10){
  d.bike.train.id <- sample(seq_len(nrow(d.bike)),size = floor(0.75*nrow(d.bike)))
  d.bike.train <- d.bike[d.bike.train.id,]
  d.bike.test <- d.bike[-d.bike.train.id,]
  
  #predict data with linear model
  lm.bike.1.train <- lm(final.model.1, data = d.bike.train)
  predicted.lm.bike.1.test <- predict(lm.bike.1.train,
                                             newdata = d.bike.test)
  r.squared.lm.bike.1 <- cor(predicted.lm.bike.1.test, d.bike.test$cnt)^2
  
  #predict data with non-linear model
  gam.bike.1.train <- gam(cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(weathersit) + s(hum) + s(temp) + s(atemp) + atemp:season + hum:season + hum:yr + atemp:mnth + temp:hr + hum:hr, data = d.bike.train)
  predicted.gam.bike.1.test <- predict(gam.bike.1.train,
                                        newdata = d.bike.test)
  r.squared.gam.bike.1 <- cor(predicted.gam.bike.1.test, d.bike.test$cnt)^2
  
  #predict data with poisson model
  poi.bike.1.train <- glm(final.model.1, data = d.bike)
  predicted.poi.bike.1.test <- predict(poi.bike.1.train,
                                        newdata = d.bike.test)
  r.squared.poi.bike.1 <- cor(predicted.poi.bike.1.test, d.bike.test$cnt)^2
}

mean(r.squared.lm.bike.1)
mean(r.squared.gam.bike.1)
mean(r.squared.poi.bike.1)
```
The result is quite close. But the best result is achieved by the poisson regressive model with an average R-Squared.

```{r include=FALSE}
d.bike.new <- subset(d.bike, select=-c(holiday, weekday, workingday, windspeed, casual, registered))
```
# 4. Trees
In this chapter a classification tree and a regression tree are used to predict cnt.  Once as class and once as discrete value. Afterwards, the regression tree model is used to analyse the bagging, random forests and boosting procedures.

## Classification Tree
To be able to predict a class using the classification tree, a class must first be created. For this purpose five classes are formed first. The classes indicate whether few (0) or many (5) bicycles were rented at the observed hour. The classes are formed according to a simple principle. The maximum observed value of cnt is divided by 5. Thus five equally large intervals are formed. Finally, the training and test data set is initialized with the classified data.

```{r}
max(d.bike.new$cnt)
max(d.bike.new$cnt)/5
for(i in 1:nrow(d.bike)){
  if(d.bike.new$cnt[i] >= 0 & d.bike.new$cnt[i] < 196){
    d.bike.new$class[i] <- 0
  } else if (d.bike.new$cnt[i] >= 196 & d.bike.new$cnt[i] < 391){
    d.bike.new$class[i] <- 1
  } else if (d.bike.new$cnt[i] >= 391 & d.bike.new$cnt[i] < 586){
    d.bike.new$class[i] <- 2
  } else if (d.bike.new$cnt[i] >= 586 & d.bike.new$cnt[i] < 781){
    d.bike.new$class[i] <- 3
  } else if (d.bike.new$cnt[i] >= 781 & d.bike.new$cnt[i] <= 977){
    d.bike.new$class[i] <- 4
  }
}

d.bike.train.id <- sample(seq_len(nrow(d.bike.new)),size = floor(0.75*nrow(d.bike.new)))
d.bike.train.new <- d.bike.new[d.bike.train.id,]
d.bike.test.new <- d.bike.new[-d.bike.train.id,]
```

On the basis of the five defined classes, a classification tree is now fitted on the training data set. That it is a classification tree is shown by the y-variable class, which is loaded into the model as a factor. Cnt is removed from the model because it correlates too strongly with class.

```{r}
tree.classification.bike.1 <- tree(as.factor(class) ~.-cnt, data = d.bike.train.new)
summary(tree.classification.bike.1)
```

The result shows the structure of the classification tree. There are 10 terminal nodes available. Not all variables were used to create the tree. Used were: hr, temp and yr. Now the tree should be plotted visually.

```{r}
plot(tree.classification.bike.1)
text(tree.classification.bike.1, pretty=1, cex=0.75)
```

As can be seen, the attribute hr is the root and therefore the most important attribute. As the summary has already shown, only hr, yr and temp are used for the classification. Furthermore, there are only 10 leaves or end nodes.

Now a prediction is to be made on the already known data, so that hits and error rate can be calculated afterwards.

```{r}
tree.classification.bike.pred.train <- predict(tree.classification.bike.1, d.bike.train.new, type="class")
(tree.classification.bike.pred.train.ct <- table(tree.classification.bike.pred.train, as.factor(d.bike.train.new$class)))
tree.classification.bike.pred.train.correct <- 0
tree.classification.bike.pred.train.error <- 0
for (i1 in 1:3) {
  for (i2 in 1:3) {
    if (i1 == i2) {
      tree.classification.bike.pred.train.correct <- tree.classification.bike.pred.train.correct + tree.classification.bike.pred.train.ct[i1,i2]
    }else{
      tree.classification.bike.pred.train.error <- tree.classification.bike.pred.train.error + tree.classification.bike.pred.train.ct[i1,i2]
    }
  }
}
(tree.classification.bike.pred.train.rate <- tree.classification.bike.pred.train.correct/sum(tree.classification.bike.pred.train.ct)) 
(tree.classification.bike.pred.train.error <- 1 - tree.classification.bike.pred.train.rate) 
```
The table shows that the classification makes many errors. For example, data records are classified as 0 although they are not class 0. The hits and error rate confirms this. About 70% of the data objects are correctly classified and about 30% are incorrectly classified. What does the result now look like on the test data set?

```{r}
tree.classification.bike.pred.test <- predict(tree.classification.bike.1, d.bike.test.new, type="class")
(tree.classification.bike.pred.test.ct <- table(tree.classification.bike.pred.test, as.factor(d.bike.test.new$class)))
tree.classification.bike.pred.test.correct <- 0
tree.classification.bike.pred.test.error <- 0
for (i1 in 1:3) {
  for (i2 in 1:3) {
    if (i1 == i2) {
      tree.classification.bike.pred.test.correct <- tree.classification.bike.pred.test.correct + tree.classification.bike.pred.test.ct[i1,i2]
    }else{
      tree.classification.bike.pred.test.error <- tree.classification.bike.pred.test.error + tree.classification.bike.pred.test.ct[i1,i2]
    }
  }
}
(tree.classification.bike.pred.test.rate <- tree.classification.bike.pred.test.correct/sum(tree.classification.bike.pred.test.ct)) 
(tree.classification.bike.pred.test.error <- 1 - tree.classification.bike.pred.test.rate) 
```

The result is basically the same. The test data set is also classified with a hit rate of 70% and an error rate of 30%. Pruning could possibly improve this result.

```{r}
tree.classification.bike.pruning <- cv.tree(tree.classification.bike.1, FUN = prune.misclass)
plot(tree.classification.bike.pruning$size, tree.classification.bike.pruning$dev, type="b")
plot(tree.classification.bike.pruning$k, tree.classification.bike.pruning$dev, type="b")
```

The graph shows that probably flattens from a size of 7. So for the parameter "best" 7 is chosen. Now the pruning can be performed.

```{r}
prune.tree.classification.bike <- prune.misclass(tree.classification.bike.1, best=7) 
summary(prune.tree.classification.bike)
```

This procedure enabled the tree to be pruned by three leaves. What does the pruned tree look like visually?

```{r}
plot(prune.tree.classification.bike)
text(prune.tree.classification.bike,pretty=0)
```

hr is still the most important attribute and therefore the root of the tree. The left branch of the initial tree now disappears completely. The right branches could also be reduced. Now the performance prediction on the test data set is to be checked, so that a comparison with the initial tree can be made.

```{r}
prune.tree.classification.bike.pred.test <- predict(prune.tree.classification.bike,  d.bike.test.new, type="class")
(prune.tree.classification.bike.pred.test.ct <- table(prune.tree.classification.bike.pred.test, as.factor(d.bike.test.new$class)))
(prune.tree.classification.bike.pred.test.correct <- sum(prune.tree.classification.bike.pred.test==as.factor(d.bike.test.new$class))/sum(prune.tree.classification.bike.pred.test.ct)) 
(prune.tree.classification.bike.pred.test.error <- 1 - prune.tree.classification.bike.pred.test.correct)
tree.classification.bike.pred.test.rate
tree.classification.bike.pred.test.error 
```

The table as well as the error and hit rates show that even the trimmed tree still makes errors in the classification. The improvement is - as can be seen from the lower two values - minimal.

## Regression Tree
Count values are not bell-shaped distributed
```{r}
hist(d.bike.train.new$cnt)
```

in this section the variable "cnt" will be predicted (--> factor)
```{r}
table(d.bike.train.new$cnt) 
```
"cnt" is a categorical variable -> normally classification tree is to be used. but since it has more than 32 levels, regression tree is to be generated. 

Proof: classification did not work with "cnt" because at most 32 levels are possible. cnt has more than 1000 levels. Following code would not work:
```{r}
#tree.classification.bike <- tree(as.factor(cnt) ~ .-class, data = d.bike.train.new) 
#tree.classification.bike.pred <- predict(tree.classification.bike, d.bike.train.new, type="class")
```
regression tree is generated:
```{r}
tree.regression.bike <- tree(cnt ~ .-class, data = d.bike.train.new) 
plot(tree.regression.bike)
text(tree.regression.bike, pretty=1, cex=0.75)
```
tree has 10 nodes. See summary:
```{r}
summary(tree.regression.bike)
```

....regression tree is used for prediction. 
prediction is made based on training data
```{r}
tree.regression.bike.train.pred <- predict(tree.regression.bike, d.bike.train.new, type="vector")
```
predictions of regression tree with true "cnt" values are compared. it shows a positive correlation
see graphic: 
```{r}
plot(tree.regression.bike.train.pred,d.bike.train.new$cnt)
abline (0 ,1) # compare with the function f(x)=x (intercept 0, slope 1)
```

errors are calculated (error residual calculation: [predicted "cnt" values] - [real "cnt" values])
```{r}
error <- tree.regression.bike.train.pred-d.bike.train.new$cnt
element_ID <- 1:length(error)
```

Analysis of the resuduals: The majority of the residents are within the frequency of 200
see graphic:
```{r}
plot(element_ID,error)
title(main="Analysis of the residuals")
abline(0 ,0, lwd=5, col="skyblue", lty="dotted")
abline(200 ,0, lwd=5, col="red", lty="dotted")
abline(-200 ,0, lwd=5, col="red", lty="dotted")
```
Histogram of error: Most errors do not seem to be over 200 up and down (as already seen in the residence analysis above).
The errors appear to be bell shaped.
see graphic: 
```{r}
hist(error)
```
some numbers on train data
RSS: 141337474
MSE: 10843.75
deviation: 104.1333
...calculated with following code:
```{r}
(RSS <- sum((d.bike.train.new["cnt"]-tree.regression.bike.train.pred)^2))
(MSE <- mean(((d.bike.train.new["cnt"]-tree.regression.bike.train.pred)^2)$cnt))
(deviation <- sqrt(MSE))
```
a prediction is made based on test data
```{r}
tree.regression.bike.test.pred <- predict(tree.regression.bike, d.bike.test.new, type="vector")
```
MSE:
An MSE comparison is made between train and test data. 
As usual, the MSE of test data is higher - but in this case only slightly higher, which shows a very good performance of the decision tree.
MSE of training data: 10728.37
MSE of test data: 10889.61
```{r}
(MSE.train <- mean(((d.bike.train.new["cnt"]-tree.regression.bike.train.pred)^2)$cnt))
(MSE.test <- mean(((d.bike.test.new["cnt"]-tree.regression.bike.test.pred)^2)$cnt))
```
Graphical comparison of error residuals between training and test data:
result: As one can see in the graph, the distribution of error rates between training and test data looks the same --> good performance of the decision tree!
see graphic: 
```{r}
errors.2.in <- predict(tree.regression.bike, d.bike.train.new, type="vector")-d.bike.train.new$cnt
element.2.in <- as.integer(names(errors.2.in))
errors.2.in_dataframe <- tibble(element.2.in,errors.2.in,"TRAIN")
colnames(errors.2.in_dataframe) <- c('ID','error','type')
errors.2 <- predict(tree.regression.bike, d.bike.test.new, type="vector")-d.bike.test.new$cnt
element.2 <- as.integer(names(errors.2))
errors.2.out_dataframe <- tibble(element.2,errors.2,"TEST")
colnames(errors.2.out_dataframe) <- c('ID','error','type')

errors.2_dataframe <- bind_rows(errors.2.in_dataframe,errors.2.out_dataframe) 
errors.2_dataframe <- arrange(errors.2_dataframe, ID)

ggplot(data = errors.2_dataframe, mapping = aes(x = ID,y = error, color = type)) + 
  geom_point() + geom_boxplot(alpha = 0.5)
```

pruning of regression tree:
The average deviance is influenced by the number of leaves. 
Up to four leaves the deviance decreases significantly. From then on the the deviance becomes stable
See graphic: 
```{r}
tree.regression.bike.pruning = cv.tree(tree.regression.bike, FUN = prune.tree)
plot(tree.regression.bike.pruning)
```
cross-validation error-rate based on size and k:

In the graphic below one can see again, that from the 4th node on, the curve flattens out:
```{r}
par(mfrow=c(1,2))
plot(tree.regression.bike.pruning$size, tree.regression.bike.pruning$dev, type="b")
```
On this graphic, one can see that after 5.0e+0.7 k, the curve flattens out: 
```{r}
plot(tree.regression.bike.pruning$k, tree.regression.bike.pruning$dev, type="b")
par(mfrow=c(1,1))
```
--> On this level the number of leaves should be chosen.

Based on the findings above, the tree was created once with 4 and once with 5 nodes. 
With the tree with 5 nodes, the goal was to find out whether the expected lower mean deviance would allow a better tree.
Result: Both trees (generated below) are good, none of them has unnecessary decisions. 
see graphics of both trees:

Tree with 4 nodes:
```{r}
tree.regression.bike.pruned <- prune.tree(tree.regression.bike, best = 5)
plot(tree.regression.bike.pruned)
text(tree.regression.bike.pruned, pretty=1, cex=0.75)
```
Tree with 5 nodes:
```{r}
tree.regression.bike.pruned2 <- prune.tree(tree.regression.bike, best = 4)
plot(tree.regression.bike.pruned2)
text(tree.regression.bike.pruned2, pretty=1, cex=0.75)
```
Summary of both trees:
mean deviance of tree with 5 leaves ("tree.regression.bike.pruned"): 14490
mean deviance of tree with 4 leaves ("tree.regression.bike.pruned2"): 15830
For the further part of the residual tree analysis (only within this section), the tree with five nodes is to be preferred, since it has a lower mean deviance.
sidenote: in the summary of the trees, one can also see that only the dimensions "hr" and "temp" are used in both trees after the prune.
summary of tree with 5 leaves:
```{r}
summary(tree.regression.bike.pruned)
```
summary of tree with 4 leaves:
```{r}
summary(tree.regression.bike.pruned2)
```

Next was about prediction tests on the recently pruned tree with five nodes. 
Result: With train data we came to an MSE of 14488.46  and with test data to 14824.47 (difference. 
--> This shows a very good performance of the tree.

Prediction using pruned tree based on training data:
```{r}
tree.regression.bike.pruned.train.pred <- predict(tree.regression.bike.pruned, d.bike.train.new, type="vector")
(MSE.pruned.train <- mean(((d.bike.train.new["cnt"]-tree.regression.bike.pruned.train.pred)^2)$cnt))
```
Prediction using pruned tree based on test data:
```{r}
tree.regression.bike.pruned.test.pred <- predict(tree.regression.bike.pruned, d.bike.test.new, type="vector")
(MSE.pruned.test <- mean(((d.bike.test.new["cnt"]-tree.regression.bike.pruned.test.pred)^2)$cnt))
```



## Bagging
In this section, the bagging shall be performed on the regression tree model to reduce the variance. As y-variable not the class is used but cnt. The number of Bootstrap replication is set to 25. It should no cross validation be used as set with parameter coob.

```{r}
bag.bike=bagging(cnt~.-class, data=d.bike.train.new, nbagg=25, coob =TRUE)
print(bag.bike)
```

The root mean squared error (RMSE) is 51.1286. The mean squared error (MSE) is therefore about 2614.

Now prediction is to be performed by means of bagging on the test set. The result is plotted.

```{r}
yhat.bag = predict(bag.bike,newdata=d.bike.test.new)
plot(yhat.bag, as.factor(d.bike.test.new$cnt))
abline(0,1)
```

The graphic representation shows us what the MSE has already shown us. The devinace from the estimate (straight line) and thus the variance is large.

```{r}
mean((yhat.bag-d.bike.test.new$cnt)^2)
```

The calculation of the MSE from the forecast confirms this a second time.

## Random Forest
At this time Random Forest could not be executed for unknown reasons. The process kept getting hung up.

## Boosting
In this section Boosting will be used to improve the model. Since our data set is count data, we choose the Poisson distribution. The number of trees to be generated in boosting is set to 1000.
```{r}
boost.bike=gbm(cnt~.-class,data=d.bike.train.new,
               distribution="poisson",n.trees=1000, interaction.depth=4)
summary(boost.bike)
```

By far the most important attribute is hr. As the result shows the attributes yr and temp are also important. This is not surprising, since the tree developed in the section "Classification Tree" was already based on these three attributes.

```{r}
plot(boost.bike ,i="hr") 
plot(boost.bike ,i="yr")
```

The graphical representation of the two most important variables hr and yr shows a clear effect on cnt. The cnt is different in the individual hours as well as in the two years.

A prediction is performed to check performance on the test data record. As in the previous examples, performance is assessed using MSE.

```{r}
yhat.boost=predict(boost.bike,newdata=d.bike.test.new, n.trees=1000)
mean((yhat.boost -d.bike.test.new$cnt)^2)
```

The MSE is very high at 66442. About thirty times as high as in the example of bagging (see Bagging). Adjustments to the shrinkage parameter have no great effect (see line 668, group13_bikesharing.R).

# 5. Support Vector Machines
The fifth chapter deals with the classification of data using Support Vector Machines (SVM). Two SVMs are trained and tested. In the first case, the data is classified into two classes using SVM with two predictors. In the second case, the data is classified in five classes using the final model.

## 2-class linear SVM with casual und atemp
An interesting classification is the division of the data in cyclists without subscription (casual) into those who ride during cool temperatures and those who ride during very warm temperatures. The two attributes casual and breath are considered. In a plot, a quite centered seperating hyperplane is supposed to divide the data into two classes. For this purpose an intercept of the x-value atemp of 0.5525 is defined straight-forward, which separates the two types of cyclist visually.

```{r}
xi <- 0.5525
ggplot(data = d.bike, mapping = aes(y = casual, x = atemp)) + geom_point() + geom_vline(xintercept = xi, color = "blue")
```

The scatterplot shows the blue hyperplane.

The next step is to classify the data so that the SVM model knows which classes to determine. This is a prerequisite in the classification. A simple if-statement can be defined for classification. If atemp is smaller than the vertical intercept defined in the visual example, it is class 0, i.e. those cyclists without a subscription who rent a bicycle even at cooler temperatures. If the observed atemp is greater than or equal to the defined intercept of 0.5525, then we are dealing with class 1, i.e. a cyclist who rents a bicycle even in warmer temperatures. A for-loop is used to perform the classification for each observed value.

```{r}
for(i in 1:nrow(d.bike)){
  if(d.bike$atemp[i] < 0.5525){
    d.bike$class[i] <- 0
  } else if (d.bike$atemp[i] >= 0.5525){
    d.bike$class[i] <- 1
  }
}

c.bike <- data.frame(x=d.bike, y=as.factor(d.bike$class))
ggplot(data = c.bike, mapping = aes(y = x.casual, x = x.atemp,  color=y)) + geom_point() + geom_vline(xintercept = xi, size = 1, alpha = 0.5)
```
The figure shows the two classes separated by color.

Now that the classes and thus the target variable are defined, an SVM model can be fitted. As can be seen from the summary, 628 support vectors are available at the cost of 10. 316 of them are in class 0 and 312 in class 1.

```{r}
svm.bike.1 <- svm(y~x.casual+x.atemp,
                  data = c.bike,
                  kernel = "linear",
                  cost = 10,
                  scale = FALSE)
summary(svm.bike.1)
plot(svm.bike.1, c.bike, x.casual~x.atemp)
```

The plot shows the classification and the support vectors.

In order to obtain the best model with optimal cost factor, a vector with different cost values is first defined and the tune-function is used.

```{r}
cost_range <-
  c(0.01,
    0.1,
    1,
    5,
    10,
    100)

tune.out <- tune(
  svm,
  y ~ x.casual+x.atemp,
  data = c.bike,
  kernel = "linear",
  ranges = list(cost = cost_range)
)
summary(tune.out)
```
As can be seen, with a cost factor of 1, an error rate of 0 is already possible.

```{r}
svm.bike.1.best <- tune.out$best.model
summary(svm.bike.1.best)
plot(svm.bike.1.best, c.bike, x.casual~x.atemp)
```
The result of the best model shows us this. The best model has an optimal cost factor of 1 and consequently there are less support vectors. It is a little more than half of the first model.

```{r}
table(predict = predict(svm.bike.1.best, c.bike),
      truth = c.bike$y)
```

As the table shows, no errors are made in the prediction and the data is correctly classified.

## 5-class linear SVM with the developed model
In the second section of this chapter, a classification into five classes based on the final.model.1 will be made. The same classes are used that were already generated in the "Classification Tree" section. Due to the multi-dimensionality (more than 3 predictors), the SVM classification cannot be visualized.

To be able to fit an SVM model, the two data sets must be transformed.The class is removed from the x-variables and assigned as factor to the y-variable, i.e. the target variable.

```{r}
d.bike.train.svm <- data.frame(x=subset(d.bike.train.new, select=-c(class)), y=as.factor(d.bike.train.new$class))
d.bike.test.svm <- data.frame(x=subset(d.bike.test.new, select=-c(class)), y=as.factor(d.bike.test.new$class))
```

Now the model can be fitted. The cost factor is deliberately set low, so that errors in the classification occur first, which are then to be eliminated with the tuning. At this point it should also be noted that the model in this example is now fitted with the training data record.

```{r}
svm.bike.2 <- svm(y ~ .,
                  data = d.bike.train.svm,
                  kernel = "linear",
                  cost = 0.01,
                  metric = "RMSE",
                  scale = FALSE,
)
summary(svm.bike.2)
```

Now the classes are predicted once on the training and once on the test data set.

```{r}
predict.svm.bike.2.train <- predict(svm.bike.2, d.bike.train.svm)
table(predict.svm.bike.2.train, d.bike.train.svm$y)

predict.svm.bike.2.test <- predict(svm.bike.2, d.bike.test.svm)
table(predict.svm.bike.2.test, d.bike.test.svm$y)
```

Errors are made on both data sets. These must now be eliminated. To do this, a cost range is initialized as in the first example. The model is again optimized using the tune function.

```{r}
cost_range <-
  c(0.01,
    0.1,
    1,
    5,
    10,
    100,
    1000)

tune.out <- tune(
  svm,
  y~.,
  data = d.bike.train.svm,
  kernel = "linear",
  ranges = list(cost = cost_range),
  scale = FALSE
  )
summary(tune.out)
```

Also in this example, a cost value of 1 seems to be sufficient to obtain an error rate of 0.

```{r}
svm.bike.2.best <- tune.out$best.model
summary(svm.bike.2.best)
```

The optimal model shows the cost value 1. In this classification "only" 59 support vectors are generated. These are distributed very differently in the classes. The middle class 2 contains only one support vector.

Subsequently, the classes are to be predicted once on the training data set and once on the test data set. Based on the ratio of correct and incorrect classifications, the performance on both data sets will be evaluated.

```{r}
predict.svm.bike.2.best.train <- predict(svm.bike.2.best, d.bike.train.svm)
table(predict.svm.bike.2.best.train, d.bike.train.svm$y)

corrects=sum(predict.svm.bike.2.best.train==d.bike.train.svm$y)
errors=sum(predict.svm.bike.2.best.train!=d.bike.train.svm$y)
(performance_train=corrects/(corrects+errors))
```

A performance rate of 100% is achieved on the training data record. This can already be seen in the table. All data is assigned to the correct classes.

```{r}
predict.svm.bike.2.best.test <- predict(svm.bike.2.best, d.bike.test.svm)
table(predict.svm.bike.2.best.test, d.bike.test.svm$y)

corrects=sum(predict.svm.bike.2.best.test==d.bike.test.svm$y)
errors=sum(predict.svm.bike.2.best.test!=d.bike.test.svm$y)
(performance_test=corrects/(corrects+errors))
```

The same result is shown on the test data set. The data is correctly classified using a new data set. An SVM can make good predictions for the two classification problems presented and the selected data set.

# 6. Neural Networks

# 7. Conclusion
The purpose of this chapter is to compare all the models that have been used in this work to analyze and predict bike rentals. The RMSE is used as comparison value respectively measure of fit. To guarantee the fairness of the comparison, a 10-fold cross validation is applied.

```{r}
lm.bike.1.rmse <- sqrt(mean(lm.bike.1$residuals^2))
gam.bike.1.rmse <- sqrt(mean(gam.bike.1$residuals^2))
poi.bike.1.rmse <- sqrt(mean(poi.bike.1$residuals^2))

#regression.tree.rmse <- mean((d.bike.test.new["cnt"]-tree.regression.bike.2.pred)$cnt)
#classification.tree.rmse <- mean((d.bike.test.new["class"]-tree.classification.bike.pred.test)$class)

bagging.rmse <- mean(yhat.bag-d.bike.test.new$cnt)
randomforest.rmse <- NA
boosting.rmse <- mean(yhat.boost -d.bike.test.new$cnt)

#svm.rmse <- rmse(svm.bike.2.best, d.bike.test.svm$y)
```