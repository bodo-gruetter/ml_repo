---
title: "Bikesharing"
output: html_document
---

# Predicting the Number of Bike Shares
In this analysis we would like to predict the count of bike rentals in Washington DC with a regression analysis, Decision Trees, Support Vector Machines (SVMs) and Neural Networks (NN).

## 0. Import Packages
Before the project starts, all required packages are imported.
```{r, message=FALSE}
library(ggplot2)
library(e1071)
library(mgcv)
library(corrplot)
library(tree)
library(mgcv)
```

```{r message=FALSE, include=FALSE}
set.seed(123)
```

## 1. Getting Data
### Data source
We downloaded the dataset "bikesharing.csv" from https://code.datasciencedojo.com/datasciencedojo/datasets/tree/master/Bike%20Sharing.

### Loading the data
In a first step the dataset is imported to R and stored in the data.frame *d.bike*:
```{r}
d.bike <- read.csv("bikesharing.csv", header=TRUE)
```

### Describing the dataset
```{r include=FALSE}
str(d.bike)
```

The dataset contains hourly information about a day and its weather conditions in 17379 observations of 17 variables. In the following, the individual attributes will be explained:

* *instant*: record index
* *dteday*: Date
* *season*: 1 = spring, 2 = summer, 3 = fall, 4 = winter
* *yr*: Year; 0: 2011, 1: 2012
* *mnth*: Month; 1 to 12
* *hr*: Hour; 0 to 23
* *holiday*: 0 = no holiday, 1 = holiday
* *weekday*: Day of the week; 1 to 7
* *workingday*: 0 = weekend or holiday, 1 = working day
* *weathersit*:
  + 1: Clear, Few clouds, Partly cloudy, Partly cloudy
  + 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
  + 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
  + 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog 
* *temp*: Normalized temperature in Celsius; The values are derived via (t-t_min)/(t_max-t_min),   t_min=-8, t_max=+39;
* *atemp*:  Normalized feeling temperature in Celsius; Normalized   feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50
* *hum*: Normalized relative humidity; The values are divided to 100 (max); 
* *windspeed*: Normalized windspeed ; The values are divided to 67 (max)
* *casual*: number of non-registered  (eg. tourists)
* *registered*: number of registered users (eg. workers that use bikes daily)
* *cnt*: number of total rentals

### Investigating the correlation between all predictors
In order to make a valid feature selection in the next step, we now want to investigate the correlation between the predictors. The so-called corrplot offers a simple and clear display. This shows a correlation matrix. Correlations are represented by dots. The exact value can be read off the scale.
```{r}
predictors = c("dteday", "season", "yr","mnth","hr","holiday","weekday",
             "workingday","weathersit","temp","atemp","hum","windspeed", "casual", "registered")
corrplot(cor(data.matrix(d.bike[predictors])))
```

As the corrplot shows, dteday correlates strongly with the predictor yr. Furthermore it can be seen that there is a correlation between dteday and season as well as mnth. There is also a relatively strong correlation between month and season.The strongest correlation of almost 1 shows temp and atemp.

### Feature Selection
We decide to delete the feature *instant* and *dteday*. *instant* is a record index that has been artificially created. Therefore it will have no relevant effect on *cnt*. *dteday* is the date of the record. It correlates with the three other attributes *season*, *yr* and *mnth*. The features season and mnth as well as temp and atem will be kept for now.

```{r}
d.bike <- subset(d.bike, select=-c(instant, dteday))
```

```{r include=FALSE}
d.bike$season <- as.factor(d.bike$season)
d.bike$yr <- as.factor(d.bike$yr)
d.bike$mnth <- as.factor(d.bike$mnth)
d.bike$hr <- as.factor(d.bike$hr)
d.bike$holiday <- as.factor(d.bike$holiday)
d.bike$weekday <- as.factor(d.bike$weekday)
d.bike$workingday <- as.factor(d.bike$workingday)
d.bike$weathersit <- as.factor(d.bike$weathersit)
```

### Checking the data
```{r}
head(d.bike)
tail(d.bike)
```

As it looks like the data set was imported completely. To check if there are any missing values (not available, NA) in this dataset we count the number of NAs in the data set.
```{r}
sum(is.na(d.bike))
mean(is.na(d.bike))
```

The record contains no rows with missing data.

### Creating training and test data set
At this point the data set d.bike is divided into training and test set. The training set should consist of 75% of the data and the test set of 25% of the data.
```{r}
d.bike.train.id <- sample(seq_len(nrow(d.bike)),size = floor(0.75*nrow(d.bike)))
d.bike.train <- d.bike[d.bike.train.id,]
d.bike.test <- d.bike[-d.bike.train.id,]
```

## 2. Explorative Data Analysis
In this section each attribute is considered to make certain assumptions. Different statistical diagrams are used for this purpose. The effects of the different attributes on cnt are tested using ANOVA for categorical attributes and a linear regression model for numerical values. The aim of this explorative data analysis is to get a starting point for the model building.

To keep our documentation clear, not all graphical analyses are shown. For example, interactions between predictors are not shown in this document but only described. At these points, references are made to the R-file.

### Count
In the very first step we would like to look at the distribution of the response variable count. For this purpose we create a histogram.
```{r}
ggplot(data = d.bike, aes(x=cnt)) +
  geom_histogram(bins=30, colour="black", ylab="Frequency") + xlab("Count") + ylab("Frequency")
mean(d.bike$cnt)
sd(d.bike$cnt)
```
As we can see, this is a right skewed gamma distribution. So in most hours - the number of bike rentals are hourly observations - between 0 and 30 bikes (the most left bar) are rented.

Bootstrapping:
```{r include=FALSE}
sort(d.bike$cnt)
```

```{r}
mean(d.bike$cnt)

id <- sample(1:length(d.bike$cnt), replace = TRUE)
mean(d.bike$cnt[id])

B <- 1000
t.mean <- c()
for(i in 1:B){
  t.id <- sample(1:length(d.bike$cnt), replace = TRUE)
  t.d.bike <- d.bike$cnt[t.id]
  t.mean[i] <- mean(t.d.bike)
}
length(t.mean)

hist(t.mean, breaks = 50)
abline(v = mean(d.bike$cnt), col = "red")

sorted.means <- sort(t.mean)
quantile(sorted.means, probs = c(0.025, 0.975))
```

### Season
If we want to visualize the effect of the *season* as a categorical variable on the number of bike rentals it is a good idea to use a box plot.
```{r}
ggplot(data = d.bike, aes(group=season, y = cnt, x = as.factor(season))) +
  geom_boxplot() + xlab("Season") + ylab("Count")
```

As the box plot shows, season 1 (spring) is particularly different from season 2, 3 and 4 (summer, autumn and winter). The median is lowest in spring, even lower than in winter. In summer, autumn and winter, the medians differ only marginally.  Since the medians differ partially, one can assume a slight effect. However, we want to examine this more closely.

With an Analysis of Variance (ANOVA) we check whether at least one level of Seasons has a significant effect on the number of bike rentals. The first step is to create a model that does not consider any effect from *season*. With an F-test we compare then both models with each other.

```{r}
lm.season.1 <- lm(cnt ~ as.factor(season), data = d.bike)
lm.season.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.season.0, lm.season.1)
```

As we can see from the p-value there is strong evidence that at least one level in *season* has a strong effect on the number of bike rentals. The lower value of the residual sums of squares in lm.season.1 also indicates that the more complex model has smaller residuals than the less complex model lm.season.0.

### Year
Since we also consider year to be a categorical variable, a box plot is again a good choice. For all categorical variables we will use a boxplot for the graphical representation in the following.

```{r}
ggplot(data = d.bike, aes(group=yr, y = cnt, x = yr)) +
  geom_boxplot()
```

At first glance we can see that the number of bike rentals has increased in year 1 (2012). The median is above the median of year 0 (2011).

```{r}
lm.year.1 <- lm(cnt ~ as.factor(yr), data = d.bike)
lm.year.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.year.0, lm.year.1)
```

The p-value shows strong evidence that one of the two years 2010 and 2011 have a significant effect on cnt. Furthermore, the RSS shows that the more complex model lm.year.1 has significantly smaller residuals than the simpler one.

### Month
```{r}
ggplot(data = d.bike, aes(group=mnth, y = cnt, x = as.factor(mnth))) +
  geom_boxplot() + scale_x_discrete(limits=seq(1,12)) + xlab("Month") + ylab("Count")
```

Here we see that the bike rental increased during the summer months May to September or when the temperatures were more pleasant. Visually, the months seem to have an effect on cnt.

```{r}
lm.month.1 <- lm(cnt ~ as.factor(mnth), data = d.bike)
lm.month.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.month.0, lm.month.1)
```

As the p-value shows, there is strong evidence that at least one month must have a significant effect on cnt. Also the RSS shows little surprising. The more complex model lm.month.1 seems to represent the data better than the model lm.month.0.


### Hour
```{r}
ggplot(data = d.bike, aes(group=hr, y = cnt, x = as.factor(hr))) +
  geom_boxplot() + scale_x_discrete(limits=seq(1,23)) + xlab("Hour") + ylab("Count")
```

The same applies to the time of day and hour. Bikes were most often rented in the morning between 7 and 8 am. and in the evening between 5 and 6 pm. The same flow also applies to the median. The individual times of day seem to differ greatly from one another.

```{r}
lm.hr.1 <- lm(cnt ~ as.factor(hr), data = d.bike)
lm.hr.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.hr.0, lm.hr.1)
```

The p-value also reflects this. At least one time of day has a significant effect on cnt. The RSS differs here extremely strongly between complex and simple models. The time of day will probably play a role in the final regression model.

### Holiday
```{r}
ggplot(data = d.bike, aes(group=holiday, y = cnt, x = as.factor(holiday))) +
  geom_boxplot() + xlab("Holiday") + ylab("Count")
```

During the holiday period, fewer bikes are rented according to the boxplot diagram. However, the difference is very pleasant compared to the normal working period. The holiday period median is close to the work time median.

```{r}
lm.holiday.1 <- lm(cnt ~ as.factor(holiday), data = d.bike)
lm.holiday.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.holiday.0, lm.holiday.1)
```

The p-value shows strong evidence that no holiday and/or holiday has a significant effect on cnt. However, if we look at RSS at this point, which differs only minimally between simple and complex model, holiday could be a feature that will be not included in the model.

### Weekday
```{r}
ggplot(data = d.bike, aes(group=weekday, y= cnt, x = as.factor(weekday))) +
  geom_boxplot() + xlab("Weekday") + ylab("Count")
```

During all seven days of the week, the bike rental is almost constant. On Monday and Sunday there are less bikes rented. The median is lowest on Monday and highest on Saturday. On all other days the median is similar.  

```{r}
lm.weekday.1 <- lm(cnt ~ as.factor(weekday), data = d.bike)
lm.weekday.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.weekday.0, lm.weekday.1)
```

As the p-value shows, there is only medium evidence that at least one weekday has a significant effect on cnt. Also the RSS does not differ too much between simple and complex model. This leads to the conclusion that probably the weekday is not included in the final model.

### Working day
```{r}
ggplot(data = d.bike, aes(group=workingday, y = cnt, x = as.factor(workingday))) +
  geom_boxplot() + xlab("Working day") + ylab("Count")
```

On working or not-working days, the count of the bike rentals are almost equal. This is also identical with the both medians.

```{r}
lm.workingday.1 <- lm(cnt ~ as.factor(workingday), data = d.bike)
lm.workingday.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.workingday.0, lm.workingday.1)
```

As the p-value shows, there is strong evidence that the no working day and/or working day has a significant effect on cnt. Again, the RSS does not differ very much. Working day possibly brings no benefit in the model and will probably be omitted.

### Weather Situation
```{r}
ggplot(data = d.bike, aes(group=weathersit, y = cnt, x = as.factor(weathersit))) +
  geom_boxplot()  + xlab("Weather Situation") + ylab("Count")
```

If we look at the weather divided on a scale between 1 and 4 as shown in the boxplot, we see that the bike rental is steadily decreasing from 1 to 4. The value 1 is the good weather and 4 the bad weather. Whereby from the value 3 away, probably already bad weather, the renting goes down faster. An identical flow can also be observed for the medians, starting from the first box plot 1 to 4.

```{r}
lm.weathersit.1 <- lm(cnt ~ as.factor(weathersit), data = d.bike)
lm.weathersit.0 <- lm(cnt ~ 1, data = d.bike)
anova(lm.weathersit.0, lm.weathersit.1)
```

There is strong evidence that at least one weather situation has a significant effect on cnt. The RSS differs greatly between the two models. This will probably be a feature that makes it into the final model.

### Temperature

```{r}
ggplot(data = d.bike, mapping = aes(y = cnt, x = temp)) +
  geom_point()  + xlab("Normalized Temp") + ylab("Count") + geom_smooth()
``` 

The representation of the regression line confirms the positive correlation. As the smoothing spline shows, temperature is a more complex relationship than linearity. It is possible that a Generalised Additive Model (GAM) could be used if temp is considered in the final model. As it seems higher temperatures lead to a higher number of rented bikes.

```{r}
library(mgcv)
gam.temp.1 <- gam(cnt ~ s(temp), data = d.bike)
summary(gam.temp.1)
``` 

First, a relatively high estimated degree of freedom (edf) of 8,747 is striking. This indicates the high complexity of the relationship. The p-value shows strong evidence that the slope is not 0 and thus there is a correlation between temp and cnt. The adjusted R-Squared shows that 17 percent of the variability is explained by the data. Thus temp could play a role in the final model. temp shows interaction effects with all of the categorical variables except the yr (see line 142, group13_bikesharing.R).

### Feeling Temperature

```{r}
ggplot(data = d.bike, mapping = aes(y = cnt, x = atemp)) +
  geom_point() + xlab("Normalized Feeling Temp") + ylab("Count") + geom_smooth()
```

Not surprisingly, atemp is also related to a higher degree of complexity. So again a GAM is useful to investigate the effect of atemp on cnt.

```{r}
gam.atemp.1 <- gam(cnt ~ s(atemp), data = d.bike)
summary(gam.atemp.1)
``` 

The edf again shows a more complex relationship between atemp and cnt. The p-value shows strong evidence that the slope is not 0. The R-Squared shows that the model explains 17% of the data. Therefore, atemp could also play a role in the final model. atemp shows interaction effects with all of the categorical variables except the yr (see line 160, group13_bikesharing.R).

### Humidity

```{r}
ggplot(data = d.bike, mapping = aes(y = cnt, x = hum)) +
  geom_point() + xlab("Humidity") + ylab("Count") + geom_smooth()
```

The smoothing spline shows a possible quadratic effect of hum on cnt. Once again, a GAM is the obvious choice.

```{r}
gam.hum.1 <- gam(cnt ~ s(hum), data = d.bike)
summary(gam.hum.1)
``` 

As the hypothesis test shows, there is strong evidence that hum has an effect on cnt. Furthermore, the R-Squared shows that the model explains 11% of the data. Thus, hum could also be relevant for the final model. hum shows interaction effects with all of the categorical variables (see line 178, group13_bikesharing.R).

### Windspeed

```{r}
ggplot(data = d.bike, mapping = aes(y = cnt, x = windspeed)) +
  geom_point() + xlab("Windspeed") + ylab("Count") + geom_smooth()
```

The graph shows that windspeed might have a linear effect on cnt. Furthermore the smoothing spline shows that there is very little influence of windspeed, because the curve is almost flat.

```{r}
lm.windspeed.1 <- lm(cnt ~ windspeed, data = d.bike)
summary(lm.windspeed.1)
``` 

The wind speed has a positive effect on cnt. This is surprising, since the observed data seems to decrease with increasing wind speed. As R-Squared shows, this regression model explains only 0.8% of the data. This indicates that the attribute can be omitted in the final model.

### Casual Users and registered users

```{r}
ggplot(data = d.bike, mapping = aes(y = cnt, x = casual)) +
  geom_point() + xlab("Casual Users") + ylab("Count") + geom_smooth()
```

```{r}
gam.casual.1 <- gam(cnt ~ s(casual), data = d.bike)
summary(gam.casual.1)
``` 

```{r}
ggplot(data = d.bike, mapping = aes(y = cnt, x = registered)) +
  geom_point() + xlab("Registered Users") + ylab("Count") + geom_smooth()
```

```{r}
lm.registered.1 <- lm(cnt ~ registered, data = d.bike)
summary(lm.registered.1)
``` 

Both casual and registered users show a positive effect and explain a large part of the data with the model. This suggests that casual and registered users need not be considered separately. Furthermore, casual and registered users are not further distinguished and are therefore not considered in the model.

### Model development

In this section, the insights gained from the exploratory data analysis and the consideration of the individual attributes should help to form a hypothesis about what the final model might look like. This hypothesis is the starting point for developing the model.

The explorative data analysis has shown that the attributes season, yr, mnth, hr, weathersit, temp, atemp and hum can probably be considered in the model. From this, the following model can be assumed:

    cnt = season + yr + mnth + hr + weathersit + temp + atemp + hum + season:temp + season:atemp + season:hum + yr:hum + mnth:temp + mnth:atemp + mnth:hum + hr:temp + hr:atemp + hr:hum + weathersit:temp + weathersit:atemp + weathersit:hum 
    
First five models are initialized. The full model with all predictors, the starting model with all interactions, the starting model without considering the interactions, the starting model without considering the polynomial effect and the starting model without considering polynomial and interaction effects.

```{r}
full.model.1 <- cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(holiday) + as.factor(weekday) + as.factor(workingday) + as.factor(weathersit) + temp + atemp + hum + windspeed + casual + registered

starting.model.1 <- cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(weathersit) + poly(hum, degree = 8.7) + poly(temp, degree = 8) + poly(atemp, degree = 8.9) + season:temp + season:atemp + season:hum + yr:hum + mnth:temp + mnth:atemp + mnth:hum + hr:temp + hr:atemp + hr:hum + weathersit:temp + weathersit:atemp + weathersit:hum

starting.model.2 <- cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(weathersit) + poly(hum, degree = 8.7) + poly(temp, degree = 8) + poly(atemp, degree = 8.9)

starting.model.3 <- cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(weathersit) + temp + atemp + hum + season:temp + season:atemp + season:hum + yr:hum + mnth:temp + mnth:atemp + mnth:hum + hr:temp + hr:atemp + hr:hum + weathersit:temp + weathersit:atemp + weathersit:hum

starting.model.4 <- cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(weathersit) + temp + atemp + hum

```

To find out which of the four starting models fits the data better, the results of the four models are first compared.

```{r}
lm.starting.model.1 <- lm(starting.model.1, data = d.bike)
lm.starting.model.2 <- lm(starting.model.2, data = d.bike)
lm.starting.model.3 <- lm(starting.model.3, data = d.bike)
lm.starting.model.4 <- lm(starting.model.4, data = d.bike)
summary(lm.starting.model.1)$adj.r.squared
summary(lm.starting.model.2)$adj.r.squared
summary(lm.starting.model.3)$adj.r.squared
summary(lm.starting.model.4)$adj.r.squared
```

If only the adjusted R-Squared is considered at this point, it is shown that the starting.model.1 with a value of about 0.747 explains the variability of the data better than the other starting models. Thus we take the starting.model.1 as a starting point for the model development.

```{r}
summary(lm.starting.model.1)
```

If now the p-value of the individual predictors and the interaction relationships in the output of starting.model.1 are considered, it becomes clear that certain attributes have no significant effect on cnt. The interaction relationships weathersit:atemp, weathersit:temp, hr:atemp and mnth:temp are deleted from the model. In the second iteration the interaction relations and predictors with a weak and a medium effect are removed from the model. These are: mnth:hum, season:temp and weathersit:atemp. The iteration was performed separately in the analysis (see lines 246-303, group13_bikesharing.R). For reasons of clarity, the predictors and interactions are removed in one pass.

```{r}
##1. iteration
lm.starting.model.1 <- update(lm.starting.model.1, . ~ . - weathersit:atemp)
lm.starting.model.1 <- update(lm.starting.model.1, . ~ . - weathersit:temp)
lm.starting.model.1 <- update(lm.starting.model.1, . ~ . - hr:atemp)
lm.starting.model.1 <- update(lm.starting.model.1, . ~ . - mnth:temp)
##2. iteration
lm.starting.model.1 <- update(lm.starting.model.1, . ~ . - mnth:hum)
lm.starting.model.1 <- update(lm.starting.model.1, . ~ . - season:temp)
lm.starting.model.1 <- update(lm.starting.model.1, . ~ . - weathersit:hum)
summary(lm.starting.model.1)
```

Now all interactions with weak and medium effect are removed from the model. atemp also shows a medium effect. Since it is present in certain interactions with strong effect, atemp is retained. The adapted R-Squared has only been reduced by 0.0003 points due to the adjustments. The final model now has an adjusted R-Squared of 0.7452, which explains about 74.5% of the data. The final model now looks as follows:

    cnt = season + yr + mnth + hr + weathersit + temp + atemp + hum + season:atemp + season:hum + yr:hum + mnth:atemp + hr:temp + hr:hum
    
The predictors temp, atemp and hum show polynomial effects.

```{r include=FALSE}
final.model.1 <- cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(weathersit) + poly(hum, degree = 8.7) + poly(temp, degree = 8) + poly(atemp, degree = 8.9) + atemp:season + hum:season + hum:yr + atemp:mnth + temp:hr + hum:hr
```

### Cross Validation
In this section, the result from the section "Model development" shall be validated by cross-validation. For this purpose, all models that played a role in the development of a final model are compared with each other. The mean R-squared of the individual models, which is calculated from 10-fold cross-validation, serves as a comparison value.

```{r, warning = FALSE}
lm.starting.model.1 <- lm(starting.model.1, data = d.bike)
lm.starting.model.2 <- lm(starting.model.2, data = d.bike)
lm.starting.model.3 <- lm(starting.model.3, data = d.bike)
lm.starting.model.4 <- lm(starting.model.4, data = d.bike)
lm.full.model.1 <- lm(full.model.1, data = d.bike)
lm.final.model.1 <- lm(final.model.1, data = d.bike)

for(i in 1:10){
  d.bike.train.id <- sample(seq_len(nrow(d.bike)),size = floor(0.75*nrow(d.bike)))
  d.bike.train <- d.bike[d.bike.train.id,]
  d.bike.test <- d.bike[-d.bike.train.id,]

  #predict data with starting model 1
  lm.starting.model.1.train <- lm(starting.model.1, data = d.bike.train)
  predicted.starting.model.1.test <- predict(lm.starting.model.1.train,
                                             newdata = d.bike.test)
  r.squared.starting.model.1 <- cor(predicted.starting.model.1.test, d.bike.test$cnt)^2
  
  #predict data with starting model 2
  lm.starting.model.2.train <- lm(starting.model.2, data = d.bike.train)
  predicted.starting.model.2.test <- predict(lm.starting.model.2.train,
                                             newdata = d.bike.test)
  r.squared.starting.model.2 <- cor(predicted.starting.model.2.test, d.bike.test$cnt)^2
  
  #predict data with starting model 3
  lm.starting.model.3.train <- lm(starting.model.3, data = d.bike.train)
  predicted.starting.model.3.test <- predict(lm.starting.model.3.train,
                                             newdata = d.bike.test)
  r.squared.starting.model.3 <- cor(predicted.starting.model.3.test, d.bike.test$cnt)^2
  
  #predict data with starting model 4
  lm.starting.model.4.train <- lm(starting.model.4, data = d.bike.train)
  predicted.starting.model.4.test <- predict(lm.starting.model.4.train,
                                             newdata = d.bike.test)
  r.squared.starting.model.4 <- cor(predicted.starting.model.4.test, d.bike.test$cnt)^2
  
  #predict data with full model
  lm.full.model.train <- lm(full.model.1, data = d.bike.train)
  predicted.full.model.test <- predict(lm.full.model.train,
                                       newdata = d.bike.test)
  r.squared.full.model <- cor(predicted.full.model.test, d.bike.test$cnt)^2
  
  #predict data with final model
  lm.final.model.train <- lm(final.model.1, data = d.bike.train)
  predicted.final.model.test <- predict(lm.final.model.train,
                                        newdata = d.bike.test)
  r.squared.final.model <- cor(predicted.final.model.test, d.bike.test$cnt)^2
}

mean(r.squared.starting.model.1)
mean(r.squared.starting.model.2)
mean(r.squared.starting.model.3)
mean(r.squared.starting.model.4)
mean(r.squared.full.model)
mean(r.squared.final.model)
```

As the comparison shows, the full.model.1 according to R-Squared is the best model. However, since the final model is the second best and much less complex than the full model, the result of the model development can be validated. For further analysis steps final.model.1 is used.

## 3. Regression Analysis
In this chapter the bike rental cnt will be predicted respectively estimated using a linear, a non-linear and a poisson regressive model. The three models will then be compared using a 10-fold cross-validation based on the R-Squared. 

### Linear Regression
```{r}
lm.bike.1 <- lm(final.model.1, data = d.bike)
summary(lm.bike.1)
```
The linear regression model explains about 74.5 of the variability of the data. This value has already been calculated in the section "Model development".

### Non-linear Regression
```{r}
gam.bike.1 <- gam(cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(weathersit) + s(hum) + s(temp) + s(atemp) + atemp:season + hum:season + hum:yr + atemp:mnth + temp:hr + hum:hr, data = d.bike)
summary(gam.bike.1)
```
The non-linear regression model shows the same result as the linear regression model. The variability of the data is explained by 74.5%.

### Poisson Regression

```{r}
poi.bike.1 <- glm(final.model.1, data = d.bike)
summary(poi.bike.1)
```

### Comparing the models
To enable a meaningful comparison between the three models, the models should train 10 times with the training set and then predict the new data. Finally, the corresponding R-Squared of all three models is compared to each other to make a statement about the best model.
```{r, warning = FALSE}
for(i in 1:10){
  d.bike.train.id <- sample(seq_len(nrow(d.bike)),size = floor(0.75*nrow(d.bike)))
  d.bike.train <- d.bike[d.bike.train.id,]
  d.bike.test <- d.bike[-d.bike.train.id,]
  
  #predict data with linear model
  lm.bike.1.train <- lm(final.model.1, data = d.bike.train)
  predicted.lm.bike.1.test <- predict(lm.bike.1.train,
                                             newdata = d.bike.test)
  r.squared.lm.bike.1 <- cor(predicted.lm.bike.1.test, d.bike.test$cnt)^2
  
  #predict data with non-linear model
  gam.bike.1.train <- gam(cnt ~ as.factor(season) + as.factor(yr) + as.factor(mnth) + as.factor(hr) + as.factor(weathersit) + s(hum) + s(temp) + s(atemp) + atemp:season + hum:season + hum:yr + atemp:mnth + temp:hr + hum:hr, data = d.bike.train)
  predicted.gam.bike.1.test <- predict(gam.bike.1.train,
                                        newdata = d.bike.test)
  r.squared.gam.bike.1 <- cor(predicted.gam.bike.1.test, d.bike.test$cnt)^2
  
  #predict data with poisson model
  poi.bike.1.train <- glm(final.model.1, data = d.bike)
  predicted.poi.bike.1.test <- predict(poi.bike.1.train,
                                        newdata = d.bike.test)
  r.squared.poi.bike.1 <- cor(predicted.poi.bike.1.test, d.bike.test$cnt)^2
}

mean(r.squared.lm.bike.1)
mean(r.squared.gam.bike.1)
mean(r.squared.poi.bike.1)
```
The result is quite close. But the best result is achieved by the poisson regressive model with an average R-Squared.

## 4. Trees

## 5. Support Vector Machines
The fifth chapter deals with the classification of data using Support Vector Machines (SVM). Two SVMs are trained and tested. In the first case, the data is classified into two classes using SVM with two predictors. In the second case, the data is classified in five classes using the final model.

### 2-class linear SVM with casual und atemp
An interesting classification is the division of the data in cyclists without subscription (casual) into those who ride during cool temperatures and those who ride during very warm temperatures. The two attributes casual and breath are considered. In a plot, a quite centered seperating hyperplane is supposed to divide the data into two classes. For this purpose an intercept of the x-value atemp of 0.5525 is defined straight-forward, which separates the two types of cyclist visually.

```{r}
xi <- 0.5525
ggplot(data = d.bike, mapping = aes(y = casual, x = atemp)) + geom_point() + geom_vline(xintercept = xi, color = "blue")
```

The scatterplot shows the blue hyperplane.

The next step is to classify the data so that the SVM model knows which classes to determine. This is a prerequisite in the classification. A simple if-statement can be defined for classification. If atemp is smaller than the vertical intercept defined in the visual example, it is class 0, i.e. those cyclists without a subscription who rent a bicycle even at cooler temperatures. If the observed atemp is greater than or equal to the defined intercept of 0.5525, then we are dealing with class 1, i.e. a cyclist who rents a bicycle even in warmer temperatures. A for-loop is used to perform the classification for each observed value.

```{r}
for(i in 1:nrow(d.bike)){
  if(d.bike$atemp[i] < 0.5525){
    d.bike$class[i] <- 0
  } else if (d.bike$atemp[i] >= 0.5525){
    d.bike$class[i] <- 1
  }
}

c.bike <- data.frame(x=d.bike, y=as.factor(d.bike$class))
ggplot(data = c.bike, mapping = aes(y = x.casual, x = x.atemp,  color=y)) + geom_point() + geom_vline(xintercept = xi, size = 1, alpha = 0.5)
```
The figure shows the two classes separated by color.

Now that the classes and thus the target variable are defined, an SVM model can be fitted. As can be seen from the summary, 628 support vectors are available at the cost of 10. 316 of them are in class 0 and 312 in class 1.

```{r}
svm.bike.1 <- svm(y~x.casual+x.atemp,
                  data = c.bike,
                  kernel = "linear",
                  cost = 10,
                  scale = FALSE)
summary(svm.bike.1)
plot(svm.bike.1, c.bike, x.casual~x.atemp)
```

The plot shows the classification and the support vectors.

In order to obtain the best model with optimal cost factor, a vector with different cost values is first defined and the tune-function is used.

```{r}
cost_range <-
  c(0.01,
    0.1,
    1,
    5,
    10,
    100)

tune.out <- tune(
  svm,
  y ~ x.casual+x.atemp,
  data = c.bike,
  kernel = "linear",
  ranges = list(cost = cost_range)
)
summary(tune.out)
```
As can be seen, with a cost factor of 1, an error rate of 0 is already possible.

```{r}
svm.bike.1.best <- tune.out$best.model
summary(svm.bike.1.best)
plot(svm.bike.1.best, c.bike, x.casual~x.atemp)
```
The result of the best model shows us this. The best model has an optimal cost factor of 1 and consequently there are less support vectors. It is a little more than half of the first model.

```{r}
table(predict = predict(svm.bike.1.best, c.bike),
      truth = c.bike$y)
```

As the table shows, no errors are made in the prediction and the data is correctly classified.

### 5-class linear SVM with the developed model
In the second section of this chapter, a classification into five classes based on the final.model.1 will be made. Due to the multi-dimensionality (more than 3 predictors), the SVM classification cannot be visualized.

```{r include=FALSE}
d.bike.new <- subset(d.bike, select=-c(holiday, weekday, workingday, temp, atemp, windspeed, casual, registered))
```

For this purpose, the five classes are formed first. The classes indicate whether few (0) or many (5) bicycles were rented at the observed hour. The classes are formed according to a simple principle. The maximum observed value of cnt is divided by 5. Thus five equally large intervals are formed. Finally, the training and test data set is initialized with the classified data.

```{r}
max(d.bike.new$cnt)
max(d.bike.new$cnt)/5
for(i in 1:nrow(d.bike)){
  if(d.bike.new$cnt[i] >= 0 & d.bike.new$cnt[i] < 196){
    d.bike.new$class[i] <- 0
  } else if (d.bike.new$cnt[i] >= 196 & d.bike.new$cnt[i] < 391){
    d.bike.new$class[i] <- 1
  } else if (d.bike.new$cnt[i] >= 391 & d.bike.new$cnt[i] < 586){
    d.bike.new$class[i] <- 2
  } else if (d.bike.new$cnt[i] >= 586 & d.bike.new$cnt[i] < 781){
    d.bike.new$class[i] <- 3
  } else if (d.bike.new$cnt[i] >= 781 & d.bike.new$cnt[i] <= 977){
    d.bike.new$class[i] <- 4
  }
}

d.bike.train.id <- sample(seq_len(nrow(d.bike.new)),size = floor(0.75*nrow(d.bike.new)))
d.bike.train.new <- d.bike.new[d.bike.train.id,]
d.bike.test.new <- d.bike.new[-d.bike.train.id,]
```

To be able to fit an SVM model, the two data sets must be transformed.The class is removed from the x-variables and assigned as factor to the y-variable, i.e. the target variable.

```{r}
d.bike.train.svm <- data.frame(x=subset(d.bike.train.new, select=-c(class)), y=as.factor(d.bike.train.new$class))
d.bike.test.svm <- data.frame(x=subset(d.bike.test.new, select=-c(class)), y=as.factor(d.bike.test.new$class))
```

Now the model can be fitted. The cost factor is deliberately set low, so that errors in the classification occur first, which are then to be eliminated with the tuning. At this point it should also be noted that the model in this example is now fitted with the training data record.

```{r}
svm.bike.2 <- svm(y ~ .,
                  data = d.bike.train.svm,
                  kernel = "linear",
                  cost = 0.01,
                  scale = FALSE,
)
summary(svm.bike.2)
```

Now the classes are predicted once on the training and once on the test data set.

```{r}
predict.svm.bike.2.train <- predict(svm.bike.2, d.bike.train.svm)
table(predict.svm.bike.2.train, d.bike.train.svm$y)

predict.svm.bike.2.test <- predict(svm.bike.2, d.bike.test.svm)
table(predict.svm.bike.2.test, d.bike.test.svm$y)
```

Errors are made on both data sets. These must now be eliminated. To do this, a cost range is initialized as in the first example. The model is again optimized using the tune function.

```{r}
cost_range <-
  c(0.01,
    0.1,
    1,
    5,
    10,
    100,
    1000)

tune.out <- tune(
  svm,
  y~.,
  data = d.bike.train.svm,
  kernel = "linear",
  ranges = list(cost = cost_range),
  scale = FALSE
  )
summary(tune.out)
```

Also in this example, a cost value of 1 seems to be sufficient to obtain an error rate of 0.

```{r}
svm.bike.2.best <- tune.out$best.model
summary(svm.bike.2.best)
```

The optimal model shows the cost value 1. In this classification "only" 59 support vectors are generated. These are distributed very differently in the classes. The middle class 2 contains only one support vector.

Subsequently, the classes are to be predicted once on the training data set and once on the test data set. Based on the ratio of correct and incorrect classifications, the performance on both data sets will be evaluated.

```{r}
predict.svm.bike.2.best.train <- predict(svm.bike.2.best, d.bike.train.svm)
table(predict.svm.bike.2.best.train, d.bike.train.svm$y)

corrects=sum(predict.svm.bike.2.best.train==d.bike.train.svm$y)
errors=sum(predict.svm.bike.2.best.train!=d.bike.train.svm$y)
(performance_train=corrects/(corrects+errors))
```

A performance rate of 100% is achieved on the training data record. This can already be seen in the table. All data is assigned to the correct classes.

```{r}
predict.svm.bike.2.best.test <- predict(svm.bike.2.best, d.bike.test.svm)
table(predict.svm.bike.2.best.test, d.bike.test.svm$y)

corrects=sum(predict.svm.bike.2.best.test==d.bike.test.svm$y)
errors=sum(predict.svm.bike.2.best.test!=d.bike.test.svm$y)
(performance_test=corrects/(corrects+errors))
```

The same result is shown on the test data set. The data is correctly classified using a new data set. An SVM can make good predictions for the two classification problems presented and the selected data set.

## 6. Neural Networks
